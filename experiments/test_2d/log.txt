************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./measure on a default named firedrake with 1 processor, by connor Fri Nov 13 09:27:58 2020
Using Petsc Development GIT revision: v3.4.2-33053-ge85aaad7c1  GIT Date: 2020-10-09 10:20:17 +0100

                         Max       Max/Min     Avg       Total
Time (sec):           2.229e+00     1.000   2.229e+00
Objects:              1.120e+02     1.000   1.120e+02
Flop:                 1.080e+06     1.000   1.080e+06  1.080e+06
Flop/sec:             4.846e+05     1.000   4.846e+05  4.846e+05
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.2046e+00  98.9%  1.8000e+05  16.7%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0%
 1:        Assemble: 2.4238e-02   1.1%  9.0000e+05  83.3%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          1 1.0 1.1683e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
BuildTwoSidedF         1 1.0 1.9312e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph             1 1.0 3.9339e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 3 1.0 3.6955e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin       4 1.0 4.0531e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         4 1.0 2.2316e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexBuFrCeLi         1 1.0 3.4409e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexBuCoFrCeLi       1 1.0 2.4962e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexInterp           1 1.0 7.6372e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0
DMPlexStratify         2 1.0 2.6288e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexSymmetrize       2 1.0 4.1184e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
CreateMesh             4 1.0 4.3179e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0
CreateFunctionSpace       1 1.0 2.4099e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 11  0  0  0  0  11  0  0  0  0     0
Mesh: reorder          1 1.0 3.2890e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
Mesh: numbering        1 1.0 2.5340e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
CreateSparsity         1 1.0 2.1892e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatZeroInitial         1 1.0 5.1033e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
ParLoopExecute         1 1.0 4.4810e-02 1.0 1.80e+05 1.0 0.0e+00 0.0e+00 0.0e+00  2 17  0  0  0   2100  0  0  0     4
ParLoop_Cells_wrap_form00_cell_integral_otherwise       2 1.0 4.5340e-03 1.0 1.80e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0 17  0  0  0   0100  0  0  0    40

--- Event Stage 1: Assemble

MatAssemblyBegin      20 1.0 2.3842e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd        20 1.0 6.4373e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries         5 1.0 1.9932e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   1  0  0  0  0     0
ParLoopExecute         5 1.0 2.1722e-02 1.0 9.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1 83  0  0  0  90100  0  0  0    41
ParLoop_Cells_wrap_form00_cell_integral_otherwise      10 1.0 2.1049e-02 1.0 9.00e+05 1.0 0.0e+00 0.0e+00 0.0e+00  1 83  0  0  0  87100  0  0  0    43
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

           Container     2              1          576     0.
              Viewer     1              0            0     0.
           Index Set    43             31       269168     0.
   IS L to G Mapping     1              0            0     0.
             Section    15              6         4272     0.
   Star Forest Graph    12              7         6944     0.
              Vector     3              0            0     0.
              Matrix     4              2         5736     0.
    GraphPartitioner     2              1          660     0.
    Distributed Mesh     6              2       490504     0.
            DM Label    15              4         2528     0.
     Discrete System     8              4         3808     0.

--- Event Stage 1: Assemble

========================================================================================================================
Average time to get PetscTime(): 4.76837e-08
#No PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: PETSC_DIR=/home/connor/repos/fireperf/venv/src/petsc PETSC_ARCH=default --download-pastix --download-eigen="/home/connor/repos/fireperf/venv/src/eigen-3.3.3.tgz " --download-scalapack --with-cxx-dialect=C++11 --download-pnetcdf --with-fortran-bindings=0 --download-metis --download-netcdf --download-hdf5 --with-shared-libraries=1 --download-superlu_dist --download-hypre --with-c2html=0 --download-ml --download-mpich --download-mumps --download-chaco --with-debugging=0 --download-suitesparse --download-ptscotch --with-zlib --download-hwloc
-----------------------------------------
Libraries compiled on 2020-11-11 14:03:59 on firedrake 
Machine characteristics: Linux-5.4.0-52-generic-x86_64-with-Ubuntu-18.04-bionic
Using PETSc directory: /home/connor/repos/fireperf/venv/src/petsc
Using PETSc arch: default
-----------------------------------------

Using C compiler: /home/connor/repos/fireperf/venv/src/petsc/default/bin/mpicc  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O   
Using Fortran compiler: /home/connor/repos/fireperf/venv/src/petsc/default/bin/mpif90  -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/home/connor/repos/fireperf/venv/src/petsc/include -I/home/connor/repos/fireperf/venv/src/petsc/default/include -I/home/connor/repos/fireperf/venv/src/petsc/default/include/eigen3
-----------------------------------------

Using C linker: /home/connor/repos/fireperf/venv/src/petsc/default/bin/mpicc
Using Fortran linker: /home/connor/repos/fireperf/venv/src/petsc/default/bin/mpif90
Using libraries: -Wl,-rpath,/home/connor/repos/fireperf/venv/src/petsc/default/lib -L/home/connor/repos/fireperf/venv/src/petsc/default/lib -lpetsc -Wl,-rpath,/home/connor/repos/fireperf/venv/src/petsc/default/lib -L/home/connor/repos/fireperf/venv/src/petsc/default/lib -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/7 -L/usr/lib/gcc/x86_64-linux-gnu/7 -Wl,-rpath,/usr/lib/x86_64-linux-gnu -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/lib/x86_64-linux-gnu -L/lib/x86_64-linux-gnu -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lscalapack -lpastix -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -lsuperlu_dist -lml -llapack -lblas -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lpthread -lhwloc -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lchaco -lmetis -lm -lz -lstdc++ -ldl -lmpifort -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lrt -lm -lrt -lstdc++ -ldl
-----------------------------------------

